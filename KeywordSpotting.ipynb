{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajay-ramasubramanian/CIFAR-10-Challenge/blob/main/KeywordSpotting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z8nXGLgfsqV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install speechbrain\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file prepare_GSC.py\n",
        "\"\"\"\n",
        "Data preparation for Google Speech Commands v0.02.\n",
        "\n",
        "Download: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "\n",
        "Author\n",
        "------\n",
        "David Raby-Pepin 2021\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from os import walk\n",
        "import glob\n",
        "import shutil\n",
        "import logging\n",
        "import torch\n",
        "import re\n",
        "import hashlib\n",
        "import copy\n",
        "import numpy as np\n",
        "from speechbrain.utils.data_utils import download_file\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except ImportError:\n",
        "    err_msg = (\n",
        "        \"The optional dependency pandas must be installed to run this recipe.\\n\"\n",
        "    )\n",
        "    err_msg += \"Install using `pip install pandas`.\\n\"\n",
        "    raise ImportError(err_msg)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "GSC_URL = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "\n",
        "# List of all the words (i.e. classes) within the GSC v2 dataset\n",
        "all_words = [\n",
        "    \"yes\",\n",
        "    \"no\",\n",
        "    \"up\",\n",
        "    \"down\",\n",
        "    \"left\",\n",
        "    \"right\",\n",
        "    \"on\",\n",
        "    \"off\",\n",
        "    \"stop\",\n",
        "    \"go\",\n",
        "    \"zero\",\n",
        "    \"one\",\n",
        "    \"two\",\n",
        "    \"three\",\n",
        "    \"four\",\n",
        "    \"five\",\n",
        "    \"six\",\n",
        "    \"seven\",\n",
        "    \"eight\",\n",
        "    \"nine\",\n",
        "    \"bed\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"dog\",\n",
        "    \"happy\",\n",
        "    \"house\",\n",
        "    \"marvin\",\n",
        "    \"sheila\",\n",
        "    \"tree\",\n",
        "    \"wow\",\n",
        "    \"backward\",\n",
        "    \"forward\",\n",
        "    \"follow\",\n",
        "    \"learn\",\n",
        "    \"visual\",\n",
        "]\n",
        "\n",
        "\n",
        "def prepare_GSC(\n",
        "    data_folder,\n",
        "    save_folder,\n",
        "    validation_percentage=10,\n",
        "    testing_percentage=10,\n",
        "    percentage_unknown=10,\n",
        "    percentage_silence=10,\n",
        "    words_wanted=[\n",
        "        \"yes\",\n",
        "        \"no\",\n",
        "        \"up\",\n",
        "        \"down\",\n",
        "        \"left\",\n",
        "        \"right\",\n",
        "        \"on\",\n",
        "        \"off\",\n",
        "        \"stop\",\n",
        "        \"go\",\n",
        "    ],\n",
        "    skip_prep=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares the Google Speech Commands V2 dataset.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    data_folder : str\n",
        "        path to dataset. If not present, it will be downloaded here.\n",
        "    save_folder: str\n",
        "        folder where to store the data manifest files.\n",
        "    validation_percentage: int\n",
        "        How much of the data set to use for validation.\n",
        "    testing_percentage: int\n",
        "        How much of the data set to use for testing.\n",
        "    percentage unknown: int.\n",
        "        How much data outside of the known (i.e wanted) words to preserve; relative to the total number of known words.\n",
        "    percentage silence: int\n",
        "        How many silence samples to generate; relative to the total number of known words.\n",
        "    skip_prep: bool\n",
        "        If True, skip data preparation.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> data_folder = '/path/to/GSC'\n",
        "    >>> prepare_GSC(data_folder)\n",
        "    \"\"\"\n",
        "\n",
        "    if skip_prep:\n",
        "        return\n",
        "\n",
        "    # If the data folders do not exist, we need to extract the data\n",
        "    if not os.path.isdir(os.path.join(data_folder, \"train-synth\")):\n",
        "        # Check for zip file and download if it doesn't exist\n",
        "        tar_location = os.path.join(data_folder, \"speech_commands_v0.02.tar.gz\")\n",
        "        if not os.path.exists(tar_location):\n",
        "            download_file(GSC_URL, tar_location, unpack=True)\n",
        "        else:\n",
        "            logger.info(\"Extracting speech_commands_v0.02.tar.gz...\")\n",
        "            shutil.unpack_archive(tar_location, data_folder)\n",
        "\n",
        "    # Define the words that we do not want to identify\n",
        "    unknown_words = list(np.setdiff1d(all_words, words_wanted))\n",
        "\n",
        "    # All metadata fields to appear within our dataset annotation files (i.e. train.csv, valid.csv, test.cvs)\n",
        "    fields = {\n",
        "        \"ID\": [],\n",
        "        \"duration\": [],\n",
        "        \"start\": [],\n",
        "        \"stop\": [],\n",
        "        \"wav\": [],\n",
        "        \"spk_id\": [],\n",
        "        \"command\": [],\n",
        "        \"transcript\": [],\n",
        "    }\n",
        "\n",
        "    splits = {\n",
        "        \"train\": copy.deepcopy(fields),\n",
        "        \"valid\": copy.deepcopy(fields),\n",
        "        \"test\": copy.deepcopy(fields),\n",
        "    }\n",
        "\n",
        "    num_known_samples_per_split = {\"train\": 0, \"valid\": 0, \"test\": 0}\n",
        "    words_wanted_parsed = False\n",
        "    commands = words_wanted + unknown_words\n",
        "    for i, command in enumerate(commands):\n",
        "        # logger.info(\"Preparing {}/{} commands...\".format(i, len(commands)))\n",
        "\n",
        "        # Indicate once all wanted words are parsed\n",
        "        if i >= len(words_wanted) and not words_wanted_parsed:\n",
        "            num_known_samples_total = np.sum(\n",
        "                list(num_known_samples_per_split.values())\n",
        "            )\n",
        "            num_unknown_samples_total = 105829 - num_known_samples_total\n",
        "            percentage_applied_to_unknown_samples = (\n",
        "                percentage_unknown * num_known_samples_total\n",
        "            ) / num_unknown_samples_total\n",
        "            words_wanted_parsed = True\n",
        "\n",
        "        # Read all files under a specific class (i.e. command)\n",
        "        files = []\n",
        "        for (dirpath, dirnames, filenames) in walk(\n",
        "            os.path.join(data_folder, command)\n",
        "        ):\n",
        "            files.extend(filenames)\n",
        "            break\n",
        "\n",
        "        # Fill in all fields with metadata for each audio sample file under a specific class\n",
        "        for filename in files:\n",
        "            # Once all wanted words are parsed, only retain the required percentage of unknown words\n",
        "            if (\n",
        "                words_wanted_parsed\n",
        "                and torch.rand(1)[0].tolist()\n",
        "                > percentage_applied_to_unknown_samples / 100\n",
        "            ):\n",
        "                continue\n",
        "\n",
        "            # select the required split (i.e. set) for the sample\n",
        "            split = which_set(\n",
        "                filename, validation_percentage, testing_percentage\n",
        "            )\n",
        "\n",
        "            splits[split][\"ID\"].append(\n",
        "                command + \"/\" + re.sub(r\".wav\", \"\", filename)\n",
        "            )\n",
        "\n",
        "            # We know that all recordings are 1 second long (i.e.16000 frames). No need to compute the duration.\n",
        "            splits[split][\"duration\"].append(1.0)\n",
        "            splits[split][\"start\"].append(0)\n",
        "            splits[split][\"stop\"].append(16000)\n",
        "\n",
        "            splits[split][\"wav\"].append(\n",
        "                os.path.join(data_folder, command, filename)\n",
        "            )\n",
        "\n",
        "            splits[split][\"spk_id\"].append(re.sub(r\"_.*\", \"\", filename))\n",
        "\n",
        "            if command in words_wanted:\n",
        "                splits[split][\"command\"].append(command)\n",
        "\n",
        "                num_known_samples_per_split[split] += 1\n",
        "            else:\n",
        "                splits[split][\"command\"].append(\"unknown\")\n",
        "\n",
        "            splits[split][\"transcript\"].append(command)\n",
        "\n",
        "    if percentage_silence > 0:\n",
        "        generate_silence_data(\n",
        "            num_known_samples_per_split,\n",
        "            splits,\n",
        "            data_folder,\n",
        "            percentage_silence=percentage_silence,\n",
        "        )\n",
        "\n",
        "    for split in splits:\n",
        "        new_filename = os.path.join(save_folder, split) + \".csv\"\n",
        "        new_df = pd.DataFrame(splits[split])\n",
        "        new_df.to_csv(new_filename, index=False)\n",
        "\n",
        "\n",
        "MAX_NUM_WAVS_PER_CLASS = 2 ** 27 - 1  # ~134M\n",
        "\n",
        "\n",
        "def which_set(filename, validation_percentage, testing_percentage):\n",
        "    \"\"\"Determines which data partition the file should belong to.\n",
        "\n",
        "  We want to keep files in the same training, validation, or testing sets even\n",
        "  if new ones are added over time. This makes it less likely that testing\n",
        "  samples will accidentally be reused in training when long runs are restarted\n",
        "  for example. To keep this stability, a hash of the filename is taken and used\n",
        "  to determine which set it should belong to. This determination only depends on\n",
        "  the name and the set proportions, so it won't change as other files are added.\n",
        "\n",
        "  It's also useful to associate particular files as related (for example words\n",
        "  spoken by the same person), so anything after '_nohash_' in a filename is\n",
        "  ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
        "  'bobby_nohash_1.wav' are always in the same set, for example.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    filename: path\n",
        "        File path of the data sample.\n",
        "    validation_percentage: int\n",
        "        How much of the data set to use for validation.\n",
        "    testing_percentage: int\n",
        "        How much of the data set to use for testing.\n",
        "\n",
        "    Returns\n",
        "    ---------\n",
        "    result: str\n",
        "        one of 'training', 'validation', or 'testing'.\n",
        "  \"\"\"\n",
        "    base_name = os.path.basename(filename)\n",
        "    # We want to ignore anything after '_nohash_' in the file name when\n",
        "    # deciding which set to put a wav in, so the data set creator has a way of\n",
        "    # grouping wavs that are close variations of each other.\n",
        "    hash_name = re.sub(r\"_nohash_.*$\", \"\", base_name).encode(\"utf-8\")\n",
        "    # This looks a bit magical, but we need to decide whether this file should\n",
        "    # go into the training, testing, or validation sets, and we want to keep\n",
        "    # existing files in the same set even if more files are subsequently\n",
        "    # added.\n",
        "    # To do that, we need a stable way of deciding based on just the file name\n",
        "    # itself, so we do a hash of that and then use that to generate a\n",
        "    # probability value that we use to assign it.\n",
        "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
        "    percentage_hash = (\n",
        "        int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)\n",
        "    ) * (100.0 / MAX_NUM_WAVS_PER_CLASS)\n",
        "    if percentage_hash < validation_percentage:\n",
        "        result = \"valid\"\n",
        "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
        "        result = \"test\"\n",
        "    else:\n",
        "        result = \"train\"\n",
        "    return result\n",
        "\n",
        "\n",
        "def generate_silence_data(\n",
        "    num_known_samples_per_split, splits, data_folder, percentage_silence=26\n",
        "):\n",
        "    \"\"\"Generates silence samples.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    num_known_samples_per_split: int\n",
        "        Total number of samples of known words for each split (i.e. set).\n",
        "    splits: str\n",
        "        Training, validation and test sets.\n",
        "    data_folder: str\n",
        "        path to dataset.\n",
        "    percentage_silence: int\n",
        "        How many silence samples to generate; relative to the total number of known words.\n",
        "  \"\"\"\n",
        "    for split in splits:\n",
        "        num_silence_samples = int(\n",
        "            (percentage_silence / 100.0) * num_known_samples_per_split[split]\n",
        "        )\n",
        "\n",
        "        # Fetch all background noise wav files used to generate silence samples\n",
        "        search_path = os.path.join(data_folder, \"_background_noise_\", \"*.wav\")\n",
        "        silence_paths = []\n",
        "        for wav_path in glob.glob(search_path):\n",
        "            # print(f'wav_path: {wav_path}')\n",
        "            silence_paths.append(wav_path)\n",
        "\n",
        "        # print(f'len of silence_paths: {len(silence_paths)}')\n",
        "        # Generate random silence samples\n",
        "        # Assumes that the pytorch seed has been defined in the HyperPyYaml file\n",
        "        num_silence_samples_per_path = int(\n",
        "            num_silence_samples / len(silence_paths)\n",
        "        )\n",
        "        for silence_path in silence_paths:\n",
        "            signal = read_audio(silence_path)\n",
        "            random_starts = (\n",
        "                (\n",
        "                    torch.rand(num_silence_samples_per_path)\n",
        "                    * (signal.shape[0] - 16001)\n",
        "                )\n",
        "                .type(torch.int)\n",
        "                .tolist()\n",
        "            )\n",
        "\n",
        "            for i, random_start in enumerate(random_starts):\n",
        "                splits[split][\"ID\"].append(\n",
        "                    re.sub(\n",
        "                        r\".wav\",\n",
        "                        \"/\" + str(random_start) + \"_\" + str(i),\n",
        "                        re.sub(r\".+?(?=_background_noise_)\", \"\", silence_path),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                splits[split][\"duration\"].append(1.0)\n",
        "                splits[split][\"start\"].append(random_start)\n",
        "                splits[split][\"stop\"].append(random_start + 16000)\n",
        "                splits[split][\"wav\"].append(silence_path)\n",
        "                splits[split][\"spk_id\"].append(None)\n",
        "                splits[split][\"command\"].append(\"silence\")\n",
        "                splits[split][\"transcript\"].append(None)\n"
      ],
      "metadata": {
        "id": "aGhyG_vdhNpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17de127e-8ccf-4f63-b239-e58fd36d32a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prepare_GSC.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file xvect.yaml\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/xvect_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequencies for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 100\n",
        "batch_size: 32\n",
        "lr: 0.001\n",
        "lr_final: 0.0001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 24\n",
        "left_frames: 0\n",
        "right_frames: 0\n",
        "deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 4\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Functions\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    left_frames: !ref <left_frames>\n",
        "    right_frames: !ref <right_frames>\n",
        "    deltas: !ref <deltas>\n",
        "\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [512, 512, 512, 512, 1500]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: 512\n",
        "\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, 512]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: 512\n",
        "    out_neurons: !ref <out_n_neurons>\n",
        "\n",
        "softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "    softmax: !ref <softmax>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "id": "21A1aIicIPXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train.py\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\"\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "        print(f' \\n computefeature type :{type(self.modules.compute_features)}')\n",
        "        # if   isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "            # Feature extraction and normalization\n",
        "        feats = self.modules.compute_features(wavs)\n",
        "        feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        # Embeddings + classifier\n",
        "        embeddings = self.modules.embedding_model(feats)\n",
        "        outputs = self.modules.classifier(embeddings)\n",
        "\n",
        "        # Ecapa model uses softmax outside of its classifer\n",
        "        if \"softmax\" in self.modules.keys():\n",
        "            outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return outputs, lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\n",
        "        \"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command, lens)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command, lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file, from_didatasets=[train_data], output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "dmIte-MSIt8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10dc44b9-644e-4398-a9aa-9aba1cbe58f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wavtovec2 hyperparameter file\n"
      ],
      "metadata": {
        "id": "qayC6qslpFOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file wav_to_vec_2.yaml\n",
        "# ################################\n",
        "# Model: Classification with xvector\n",
        "# Authors: Hwidong Na & Mirco Ravanelli\n",
        "#          Script adapted by David Raby-Pepin 2021\n",
        "# ################################\n",
        "\n",
        "# Basic parameters\n",
        "seed: 1986\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Use 12 for V2 12 task and 35 for V2 35 task\n",
        "number_of_commands: 12\n",
        "output_folder: !ref results/xvect_v<number_of_commands>/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "\n",
        "# Data for augmentation\n",
        "NOISE_DATASET_URL: https://www.dropbox.com/scl/fi/a09pj97s5ifan81dqhi4n/noises.zip?rlkey=j8b0n9kdjdr32o1f06t0cw5b7&dl=1\n",
        "RIR_DATASET_URL: https://www.dropbox.com/scl/fi/linhy77c36mu10965a836/RIRs.zip?rlkey=pg9cu8vrpn2u173vhiqyu743u&dl=1\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g. /path/to/GSC\n",
        "data_folder_noise: !ref <data_folder>/noise # The noisy sequencies for data augmentation will automatically be downloaded here.\n",
        "data_folder_rir: !ref <data_folder>/rir # The impulse responses used for data augmentation will automatically be downloaded here.\n",
        "train_annotation: !ref <output_folder>/train.csv\n",
        "valid_annotation: !ref <output_folder>/valid.csv\n",
        "test_annotation: !ref <output_folder>/test.csv\n",
        "noise_annotation: !ref <save_folder>/noise.csv\n",
        "rir_annotation: !ref <save_folder>/rir.csv\n",
        "\n",
        "# Percentage of files used for validation and test\n",
        "validation_percentage: 10\n",
        "testing_percentage: 10\n",
        "\n",
        "# Percentage of unknown and silence examples\n",
        "# (relative to total of known word samples) to include\n",
        "percentage_unknown: 10 # Set this to 0 for the V2 35 task\n",
        "percentage_silence: 10 # Set this to 0 for the V2 35 task\n",
        "\n",
        "skip_prep: False\n",
        "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 1\n",
        "batch_size: 4\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "\n",
        "sample_rate: 16000\n",
        "shuffle: True\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "# Feature parameters\n",
        "# n_mels: 24\n",
        "# left_frames: 0\n",
        "# right_frames: 0\n",
        "# deltas: False\n",
        "\n",
        "# Number of classes (i.e. different commands)\n",
        "out_n_neurons: !ref <number_of_commands>  #includes core commands & auxiliary words\n",
        "\n",
        "num_workers: 4\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: !ref <shuffle>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "encoder_dim: 768\n",
        "# Functions\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of noisy sequences for augmentation\n",
        "prepare_noise_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <NOISE_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_noise>\n",
        "    ext: wav\n",
        "    csv_file: !ref <noise_annotation>\n",
        "\n",
        "# Add noise to input signal\n",
        "snr_low: 0  # Min SNR for noise augmentation\n",
        "snr_high: 15  # Max SNR for noise augmentation\n",
        "\n",
        "add_noise: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    csv_file: !ref <noise_annotation>\n",
        "    snr_low: !ref <snr_low>\n",
        "    snr_high: !ref <snr_high>\n",
        "    noise_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "\n",
        "# Download and prepare the dataset of room impulse responses for augmentation\n",
        "prepare_rir_data: !name:speechbrain.augment.preparation.prepare_dataset_from_URL\n",
        "    URL: !ref <RIR_DATASET_URL>\n",
        "    dest_folder: !ref <data_folder_rir>\n",
        "    ext: wav\n",
        "    csv_file: !ref <rir_annotation>\n",
        "\n",
        "# Add reverberation to input signal\n",
        "add_reverb: !new:speechbrain.augment.time_domain.AddReverb\n",
        "    csv_file: !ref <rir_annotation>\n",
        "    reverb_sample_rate: !ref <sample_rate>\n",
        "    clean_sample_rate: !ref <sample_rate>\n",
        "    num_workers: !ref <num_workers>\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq_low: 0  # Min frequency band dropout probability\n",
        "drop_freq_high: 1  # Max frequency band dropout probability\n",
        "drop_freq_count_low: 1  # Min number of frequency bands to drop\n",
        "drop_freq_count_high: 3  # Max number of frequency bands to drop\n",
        "drop_freq_width: 0.05  # Width of frequency bands to drop\n",
        "\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: !ref <drop_freq_low>\n",
        "    drop_freq_high: !ref <drop_freq_high>\n",
        "    drop_freq_count_low: !ref <drop_freq_count_low>\n",
        "    drop_freq_count_high: !ref <drop_freq_count_high>\n",
        "    drop_freq_width: !ref <drop_freq_width>\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk_count_low: 1  # Min number of audio chunks to drop\n",
        "drop_chunk_count_high: 5  # Max number of audio chunks to drop\n",
        "drop_chunk_length_low: 1000  # Min length of audio chunks to drop\n",
        "drop_chunk_length_high: 2000  # Max length of audio chunks to drop\n",
        "\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: !ref <drop_chunk_length_low>\n",
        "    drop_length_high: !ref <drop_chunk_length_high>\n",
        "    drop_count_low: !ref <drop_chunk_count_low>\n",
        "    drop_count_high: !ref <drop_chunk_count_high>\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    repeat_augment: 1\n",
        "    shuffle_augmentations: False\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <add_noise>,\n",
        "        !ref <add_reverb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: sentence\n",
        "    std_norm: False\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "\n",
        "# Cost + optimization\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "# compute_error: !name:speechbrain.nnet.losses.classification_error\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "    weight_decay: 0.000002\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "# Logging + checkpoints\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing_output: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "id": "pOtp6T1QbhaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe4909f-552c-4515-c822-701ec9310413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting wav_to_vec_2.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train.py\n",
        "#!/usr/bin/python3\n",
        "\"\"\"Recipe for training a classifier using the\n",
        "Google Speech Commands v0.02 Dataset.\n",
        "\n",
        "To run this recipe, use the following command:\n",
        "> python train.py {hyperparameter_file}\n",
        "\n",
        "Using your own hyperparameter file or one of the following:\n",
        "    hyperparams/xvect.yaml (xvector system)\n",
        "\n",
        "Author\n",
        "    * Mirco Ravanelli 2020\n",
        "    * Hwidong Na 2020\n",
        "    * Nauman Dawalatabad 2020\n",
        "    * Sarthak Yadav 2022\n",
        "    Script adapted by David Raby-Pepin 2021\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain.nnet.CNN\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "\n",
        "\n",
        "class SpeakerBrain(sb.core.Brain):\n",
        "    \"\"\"Class for GSC training\"\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + command classifier.\n",
        "        Data augmentation and environmental corruption are applied to the\n",
        "        input speech.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Add waveform augmentation if specified.\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, lens = self.hparams.wav_augment(wavs, lens)\n",
        "        # print(f' \\n computefeature type :{type(self.modules.compute_features)}')\n",
        "        # if   isinstance(\n",
        "        #     self.modules.compute_features, speechbrain.lobes.features.Leaf\n",
        "        # ):\n",
        "        #     # if leaf, first normalize the wavs before feeding them to leaf\n",
        "        #     # no normalization is needed after LEAF\n",
        "        #     feats = self.modules.mean_var_norm(wavs, lens)\n",
        "        #     feats = self.modules.compute_features(feats)\n",
        "        # else:\n",
        "            # Feature extraction and normalization\n",
        "        feats = self.modules.ssl_model(wavs, lens)\n",
        "        feats = self.hparams.avg_pool(feats, lens)\n",
        "\n",
        "        feats= feats.view(feats.shape[0],-1)\n",
        "\n",
        "        feats = self.modules.output_mlp(feats)\n",
        "        feats = self.hparams.log_softmax(feats)\n",
        "        # print(f'\\n\\n feature size : {feats.shape}')\n",
        "\n",
        "\n",
        "        # # Ecapa model uses softmax outside of its classifer\n",
        "        # if \"softmax\" in self.modules.keys():\n",
        "        #     outputs = self.modules.softmax(outputs)\n",
        "\n",
        "        return feats, lens\n",
        "    # def compute_objectives(self, predictions, batch, stage):\n",
        "    #     \"\"\"Computes the loss using speaker-id as label.\n",
        "    #     \"\"\"\n",
        "    #     command, _ = batch.command_encoded\n",
        "\n",
        "    #     \"\"\"to meet the input form of nll loss\"\"\"\n",
        "    #     command = command.squeeze(1)\n",
        "    #     print(f'/n predictions shape: {predictions.shape}')\n",
        "    #     loss = self.hparams.compute_cost(predictions, command)\n",
        "    #     if stage != sb.Stage.TRAIN:\n",
        "    #         self.error_metrics.append(batch.id, predictions, command)\n",
        "\n",
        "    #     return loss\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using command-id as label.\n",
        "        \"\"\"\n",
        "        predictions, lens = predictions\n",
        "        uttid = batch.id\n",
        "        command, _ = batch.command_encoded\n",
        "        # print(f'command shape :{command.shape}')\n",
        "        # Concatenate labels (due to data augmentation)\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            command = self.hparams.wav_augment.replicate_labels(command)\n",
        "\n",
        "        command = command.squeeze(1)\n",
        "        # compute the cost function\n",
        "        loss = self.hparams.compute_cost(predictions, command, lens)\n",
        "        # loss = sb.nnet.losses.nll_loss(predictions, command, lens)\n",
        "\n",
        "        if hasattr(self.hparams.lr_annealing, \"on_batch_end\"):\n",
        "            self.hparams.lr_annealing.on_batch_end(self.optimizer)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(uttid, predictions, command, lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of an epoch.\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"ErrorRate\"] = self.error_metrics.summarize(\"average\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"ErrorRate\": stage_stats[\"ErrorRate\"]},\n",
        "                min_keys=[\"ErrorRate\"],\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"Creates the datasets and their data processing pipelines.\"\n",
        "\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    # 1. Declarations:\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_annotation\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\", \"start\", \"stop\", \"duration\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav, start, stop, duration):\n",
        "        start = int(start)\n",
        "        stop = int(stop)\n",
        "        num_frames = stop - start\n",
        "        sig, fs = torchaudio.load(\n",
        "            wav, num_frames=num_frames, frame_offset=start\n",
        "        )\n",
        "        sig = sig.transpose(0, 1).squeeze(1)\n",
        "        return sig\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"command\")\n",
        "    @sb.utils.data_pipeline.provides(\"command\", \"command_encoded\")\n",
        "    def label_pipeline(command):\n",
        "        yield command\n",
        "        command_encoded = label_encoder.encode_sequence_torch([command])\n",
        "        yield command_encoded\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, label_pipeline)\n",
        "\n",
        "    # 3. Fit encoder:\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file, from_didatasets=[train_data], output_key=\"command\",\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"command_encoded\"]\n",
        "    )\n",
        "\n",
        "    return train_data, valid_data, test_data, label_encoder\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # This flag enables the inbuilt cudnn auto-tuner\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # CLI:\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training)\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.core.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Dataset prep (parsing GSC and annotation into csv files)\n",
        "    from prepare_GSC import prepare_GSC\n",
        "\n",
        "    # Known words for V2 12 and V2 35 sets\n",
        "    if hparams[\"number_of_commands\"] == 12:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "        ]\n",
        "    elif hparams[\"number_of_commands\"] == 35:\n",
        "        words_wanted = [\n",
        "            \"yes\",\n",
        "            \"no\",\n",
        "            \"up\",\n",
        "            \"down\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"on\",\n",
        "            \"off\",\n",
        "            \"stop\",\n",
        "            \"go\",\n",
        "            \"zero\",\n",
        "            \"one\",\n",
        "            \"two\",\n",
        "            \"three\",\n",
        "            \"four\",\n",
        "            \"five\",\n",
        "            \"six\",\n",
        "            \"seven\",\n",
        "            \"eight\",\n",
        "            \"nine\",\n",
        "            \"bed\",\n",
        "            \"bird\",\n",
        "            \"cat\",\n",
        "            \"dog\",\n",
        "            \"happy\",\n",
        "            \"house\",\n",
        "            \"marvin\",\n",
        "            \"sheila\",\n",
        "            \"tree\",\n",
        "            \"wow\",\n",
        "            \"backward\",\n",
        "            \"forward\",\n",
        "            \"follow\",\n",
        "            \"learn\",\n",
        "            \"visual\",\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"number_of_commands must be 12 or 35\")\n",
        "\n",
        "    # Data preparation\n",
        "    run_on_main(\n",
        "        prepare_GSC,\n",
        "        kwargs={\n",
        "            \"data_folder\": hparams[\"data_folder\"],\n",
        "            \"save_folder\": hparams[\"output_folder\"],\n",
        "            \"validation_percentage\": hparams[\"validation_percentage\"],\n",
        "            \"testing_percentage\": hparams[\"testing_percentage\"],\n",
        "            \"percentage_unknown\": hparams[\"percentage_unknown\"],\n",
        "            \"percentage_silence\": hparams[\"percentage_silence\"],\n",
        "            \"words_wanted\": words_wanted,\n",
        "            \"skip_prep\": hparams[\"skip_prep\"],\n",
        "        },\n",
        "    )\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_noise_data\"])\n",
        "    sb.utils.distributed.run_on_main(hparams[\"prepare_rir_data\"])\n",
        "\n",
        "    # Dataset IO prep: creating Dataset objects and proper encodings for phones\n",
        "    train_data, valid_data, test_data, label_encoder = dataio_prep(hparams)\n",
        "\n",
        "    # Brain class initialization\n",
        "    speaker_brain = SpeakerBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # with torch.autograd.detect_anomaly():\n",
        "    # Training\n",
        "    speaker_brain.fit(\n",
        "        speaker_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = speaker_brain.evaluate(\n",
        "        test_set=test_data,\n",
        "        min_key=\"ErrorRate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-i-_jowpWV0",
        "outputId": "677bd00e-5cae-4186-8896-cc7182e57e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py wav_to_vec_2.yaml --data_folder=/path_to_/GSC\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-8aEFCCmC2s",
        "outputId": "2cdb1e8d-0d12-42c7-f2ad-ab9db533ebea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/xvect_v12/1986\n",
            "numexpr.utils - NumExpr defaulting to 2 threads.\n",
            "prepare_GSC - Extracting speech_commands_v0.02.tar.gz...\n",
            "wav_path: /path_to_/GSC/_background_noise_/pink_noise.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/white_noise.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/doing_the_dishes.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/dude_miaowing.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/running_tap.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/exercise_bike.wav\n",
            "len of silence_paths: 6\n",
            "wav_path: /path_to_/GSC/_background_noise_/pink_noise.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/white_noise.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/doing_the_dishes.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/dude_miaowing.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/running_tap.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/exercise_bike.wav\n",
            "len of silence_paths: 6\n",
            "wav_path: /path_to_/GSC/_background_noise_/pink_noise.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/white_noise.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/doing_the_dishes.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/dude_miaowing.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/running_tap.wav\n",
            "wav_path: /path_to_/GSC/_background_noise_/exercise_bike.wav\n",
            "len of silence_paths: 6\n",
            "/path_to_/GSC/noise/data.zip exists. Skipping download\n",
            "/path_to_/GSC/rir/data.zip exists. Skipping download\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "speechbrain.core - 90.2M trainable parameters in SpeakerBrain\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "  0% 0/9240 [00:05<?, ?it/s]\n",
            "speechbrain.core - Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 318, in <module>\n",
            "    speaker_brain.fit(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1555, in fit\n",
            "    self._fit_train(train_set=train_set, epoch=epoch, enable=enable)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1384, in _fit_train\n",
            "    loss = self.fit_batch(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/core.py\", line 1179, in fit_batch\n",
            "    loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
            "  File \"/content/train.py\", line 95, in compute_objectives\n",
            "    loss = self.hparams.compute_cost(predictions, command, lens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/losses.py\", line 453, in nll_loss\n",
            "    return compute_masked_loss(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/losses.py\", line 785, in compute_masked_loss\n",
            "    mask = compute_length_mask(mask_data, length)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/speechbrain/nnet/losses.py\", line 870, in compute_length_mask\n",
            "    length * data.shape[len_dim], max_len=data.shape[len_dim],\n",
            "IndexError: tuple index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZ51wn6hmMfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}